{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/grads/q/quamer.waris/anaconda3/envs/autospeech/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grads/q/quamer.waris/anaconda3/envs/autospeech/lib/python3.8/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n",
      "/home/grads/q/quamer.waris/anaconda3/envs/autospeech/lib/python3.8/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded encoder \"pretrained.pt\" trained to step 1564501\n",
      "Loaded encoder \"encoder_accent.pt\" trained to step 90001\n",
      "Found synthesizer \"Accetron_train_parallel\" trained to step 204001\n",
      "Found synthesizer \"translator_train\" trained to step 294001\n",
      "Building Wave-RNN\n",
      "Trainable Parameters: 4.481M\n",
      "Loading model weights at /home/grads/q/quamer.waris/projects/Accentron/pretrained_model/pretrained/vocoder/saved_models/pretrained/pretrained.pt\n"
     ]
    }
   ],
   "source": [
    "from synthesizer.inference import Synthesizer\n",
    "from synthesizer_like_translator.inference import Synthesizer as Translator\n",
    "from synthesizer.kaldi_interface import KaldiInterface\n",
    "from encoder import inference as encoder\n",
    "from encoder import inference_accent as encoder_accent\n",
    "from vocoder import inference as vocoder\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from utils.argutils import print_args\n",
    "import random\n",
    "import IPython.display as ipd\n",
    "from synthesizer.hparams import hparams\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import offsetbox\n",
    "from sklearn import (manifold, datasets, decomposition, ensemble, discriminant_analysis, random_projection)\n",
    "\n",
    "\n",
    "encoder_speaker_weights = Path(\"/home/grads/q/quamer.waris/projects/Accentron/pretrained_model/pretrained/encoder/saved_models/pretrained.pt\")\n",
    "encoder_accent_weights = Path(\"/home/grads/q/quamer.waris/projects/Accentron/pretrained_model/pretrained/encoder/saved_models/encoder_accent.pt\")\n",
    "vocoder_weights = Path(\"/home/grads/q/quamer.waris/projects/Accentron/pretrained_model/pretrained/vocoder/saved_models/pretrained/pretrained.pt\")\n",
    "#syn_dir = Path('/mnt/data1/waris/model_outputs/accentron/parallel_vctk_mic2/logs-Accetron_train_parallel_vctk/taco_pretrained')\n",
    "syn_dir = Path(\"/mnt/data1/waris/model_outputs/accentron/parallel/logs-Accetron_train_parallel/taco_pretrained\")\n",
    "#syn_dir_trans = Path(\"/mnt/data1/waris/model_outputs/translator/sythesizer_like/logs-translator_train/taco_pretrained\")\n",
    "syn_dir_trans = Path(\"/mnt/data1/waris/model_outputs/translator/sythesizer_like_train_set/logs-translator_train/taco_pretrained\")\n",
    "\n",
    "encoder.load_model(encoder_speaker_weights)\n",
    "encoder_accent.load_model(encoder_accent_weights)\n",
    "synthesizer = Synthesizer(syn_dir)\n",
    "translator = Translator(syn_dir_trans)\n",
    "vocoder.load_model(vocoder_weights)\n",
    "#hparams = hparams.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize(bnf, embed):\n",
    "    spec = synthesizer.synthesize_spectrograms([bnf], [embed])[0]\n",
    "    # print(spec.shape)\n",
    "    generated_wav = vocoder.infer_waveform(spec)\n",
    "    generated_wav = np.pad(generated_wav, (0, synthesizer.sample_rate), mode=\"constant\")\n",
    "    return generated_wav\n",
    "\n",
    "def translate_ppg(bnf, embed):\n",
    "    spec = translator.synthesize_spectrograms([bnf], [embed])[0]\n",
    "    return spec\n",
    "\n",
    "def generate_accent_embed(src_utterance_path):\n",
    "    wav, _ = librosa.load(src_utterance_path, hparams.sample_rate)\n",
    "    wav = encoder.preprocess_wav(wav)\n",
    "    embed_accent = encoder_accent.embed_utterance(wav)\n",
    "\n",
    "    return embed_accent\n",
    "\n",
    "def generate_speaker_embed(tgt_utterance_path):\n",
    "    wav, _ = librosa.load(tgt_utterance_path, hparams.sample_rate)\n",
    "    wav = encoder.preprocess_wav(wav)\n",
    "    embed_speaker = encoder.embed_utterance(wav)\n",
    "\n",
    "    return embed_speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_utterances = ['/mnt/data1/waris/datasets/vctk/wav48_silence_trimmed/p226/wav/',\n",
    "                        '/mnt/data1/waris/datasets/vctk/wav48_silence_trimmed/p227/wav/',\n",
    "                        '/mnt/data1/waris/datasets/vctk/wav48_silence_trimmed/p232/wav/',\n",
    "                        '/mnt/data1/waris/datasets/vctk/wav48_silence_trimmed/p256/wav/',\n",
    "                        '/mnt/data1/waris/datasets/vctk/wav48_silence_trimmed/p260/wav/',\n",
    "                        '/mnt/data1/waris/datasets/vctk/wav48_silence_trimmed/p263/wav/']\n",
    "\n",
    "japanese_utterances = ['/mnt/data1/waris/datasets/jvs_ver1/jvs001/parallel100/wav24kHz16bit/',\n",
    "                        '/mnt/data1/waris/datasets/jvs_ver1/jvs003/parallel100/wav24kHz16bit/',\n",
    "                        '/mnt/data1/waris/datasets/jvs_ver1/jvs005/parallel100/wav24kHz16bit/',\n",
    "                        '/mnt/data1/waris/datasets/jvs_ver1/jvs006/parallel100/wav24kHz16bit/',\n",
    "                        '/mnt/data1/waris/datasets/jvs_ver1/jvs013/parallel100/wav24kHz16bit/',\n",
    "                        '/mnt/data1/waris/datasets/jvs_ver1/jvs028/parallel100/wav24kHz16bit/']\n",
    "\n",
    "mandarian_utterances = ['/mnt/data1/waris/datasets/UEDIN_mandarin_bi_data_2010/downsampled_22kHz/Mandarin_talkers/Male/MM1/Man/',\n",
    "                        '/mnt/data1/waris/datasets/UEDIN_mandarin_bi_data_2010/downsampled_22kHz/Mandarin_talkers/Male/MM2/Man/',\n",
    "                        '/mnt/data1/waris/datasets/UEDIN_mandarin_bi_data_2010/downsampled_22kHz/Mandarin_talkers/Male/MM3/Man/',\n",
    "                        '/mnt/data1/waris/datasets/UEDIN_mandarin_bi_data_2010/downsampled_22kHz/Mandarin_talkers/Male/MM4/Man/',\n",
    "                        '/mnt/data1/waris/datasets/UEDIN_mandarin_bi_data_2010/downsampled_22kHz/Mandarin_talkers/Male/MM5/Man/',\n",
    "                        '/mnt/data1/waris/datasets/UEDIN_mandarin_bi_data_2010/downsampled_22kHz/Mandarin_talkers/Male/MM6/Man/']\n",
    "\n",
    "mandarian_eng_utterances = ['/mnt/data1/waris/datasets/mandarin_bi_speakers/English/MM1/wav/',\n",
    "                        '/mnt/data1/waris/datasets/mandarin_bi_speakers/English/MM2/wav/',\n",
    "                        '/mnt/data1/waris/datasets/mandarin_bi_speakers/English/MM3/wav/',\n",
    "                        '/mnt/data1/waris/datasets/mandarin_bi_speakers/English/MM4/wav/',\n",
    "                        '/mnt/data1/waris/datasets/mandarin_bi_speakers/English/MM5/wav/',\n",
    "                        '/mnt/data1/waris/datasets/mandarin_bi_speakers/English/MM6/wav/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_embeddings = []\n",
    "speaker_labels = []\n",
    "\n",
    "for utterance in english_utterances:\n",
    "    path_eng, dir_eng, files_eng = next(os.walk(utterance))\n",
    "    count = 0\n",
    "    for i, file in enumerate(files_eng):\n",
    "        if count>=100:\n",
    "            break\n",
    "\n",
    "        if \"_mic2.wav\" in file: \n",
    "            embed_speaker = generate_speaker_embed(path_eng + file)\n",
    "            speaker_embeddings.append(embed_speaker)\n",
    "            speaker_labels.append(\"ENG\")\n",
    "            count=count+1\n",
    "\n",
    "\n",
    "for utterance in japanese_utterances:\n",
    "    path_jap, dir_jap, files_jap = next(os.walk(utterance))\n",
    "\n",
    "    for i, file in enumerate(files_jap):\n",
    "        embed_speaker = generate_speaker_embed(path_jap + file)\n",
    "        speaker_embeddings.append(embed_speaker)\n",
    "        speaker_labels.append(\"JAP\")\n",
    "\n",
    "for utterance in mandarian_utterances:\n",
    "    path_man, dir_man, files_man = next(os.walk(utterance))\n",
    "    count = 0\n",
    "    for i, file in enumerate(files_man):\n",
    "        if count>=100:\n",
    "            break\n",
    "        if \"_1.wav\" in file: \n",
    "            embed_speaker = generate_speaker_embed(path_man + file)\n",
    "            speaker_embeddings.append(embed_speaker)\n",
    "            speaker_labels.append(\"MAN\")\n",
    "            count=count+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1800"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_embeddings_copy = speaker_embeddings\n",
    "speaker_labels_copy = speaker_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for utterance in mandarian_eng_utterances:\n",
    "    path_man, dir_man, files_man = next(os.walk(utterance))\n",
    "    count = 0\n",
    "    for i, file in enumerate(files_man):\n",
    "        if count>=10:\n",
    "            break\n",
    "        if \"_1.wav\" in file: \n",
    "            embed_speaker = generate_speaker_embed(path_man + file)\n",
    "            speaker_embeddings.append(embed_speaker)\n",
    "            speaker_labels.append(\"MAN_ENG\")\n",
    "            count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PCA projection\n",
      "Computing t-SNE embedding\n"
     ]
    }
   ],
   "source": [
    "### Computing PCA\n",
    "print(\"Computing PCA projection\")\n",
    "X_pca = decomposition.TruncatedSVD(n_components=2).fit_transform(speaker_embeddings)\n",
    "## Computing t-SNE\n",
    "print(\"Computing t-SNE embedding\")\n",
    "tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)\n",
    "speaker_tsne = tsne.fit_transform(speaker_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['tab:blue', 'tab:green', 'tab:red', 'tab:gray']\n",
    "shapes = [\"o\", \"^\", \"*\", \"s\"]\n",
    "labels = [\"ENG\", \"JAP\", \"MAN\", \"MAN_ENG\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 2) ENG\n",
      "(600, 2) JAP\n",
      "(600, 2) MAN\n",
      "(60, 2) MAN_ENG\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.cm as cm\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "for speaker, c, m in zip(labels, colors, shapes):\n",
    "    X_speaker_embedding = X_pca[np.where(speaker==np.array(speaker_labels))]\n",
    "    print(X_speaker_embedding.shape, speaker)\n",
    "    plt.scatter(X_speaker_embedding[:,0], X_speaker_embedding[:,1], label=speaker, marker=m, color=c)\n",
    "    # plt.scatter(speaker_embeddings[:, 0], speaker_embeddings[:, 1], c=speakers_all)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"SpeakerEmbeddings_Jap_Eng_BiMan_PCA.png\", format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 2) ENG\n",
      "(600, 2) JAP\n",
      "(600, 2) MAN\n",
      "(60, 2) MAN_ENG\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.cm as cm\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "for speaker, c, m in zip(labels, colors, shapes):\n",
    "    X_speaker_embedding = speaker_tsne[np.where(speaker==np.array(speaker_labels))]\n",
    "    print(X_speaker_embedding.shape, speaker)\n",
    "    plt.scatter(X_speaker_embedding[:,0], X_speaker_embedding[:,1], label=speaker, marker=m, color=c)\n",
    "    # plt.scatter(speaker_embeddings[:, 0], speaker_embeddings[:, 1], c=speakers_all)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"SpeakerEmbeddings_Jap_Eng_BiMan_TSNE.png\", format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{| ████████████████ 47500/48000 | Batch Size: 5 | Gen Rate: 5.1kHz | } | }"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# utterance_ids = ['VOICEACTRESS100_001', 'VOICEACTRESS100_002', 'VOICEACTRESS100_003', 'VOICEACTRESS100_004', 'VOICEACTRESS100_005', 'VOICEACTRESS100_006']\n",
    "# speakers = ['jvs001', 'jvs003']\n",
    "\n",
    "speakers = ['MM1', 'MM2','MM3', 'MM4', 'MM5', 'MM6']\n",
    "\n",
    "# for utterance in mandarian_eng_utterances:\n",
    "#     path_man, dir_man, files_man = next(os.walk(utterance))\n",
    "#     count = 0\n",
    "#     for i, file in enumerate(files_man):\n",
    "#         if count>=100:\n",
    "#             break\n",
    "#         if \"_1.wav\" in file: \n",
    "#             embed_speaker = generate_speaker_embed(path_man + file)\n",
    "#             speaker_embeddings.append(embed_speaker)\n",
    "#             speaker_labels.append(\"MAN\")\n",
    "#             count=count+1\n",
    "\n",
    "for speaker in speakers:\n",
    "    utterance_fpath = '/mnt/data1/waris/datasets/mandarin_bi_speakers/English/'+speaker+'/wav'\n",
    "    _, _, utterance_ids = next(os.walk(utterance_fpath))\n",
    "    count = 0\n",
    "    for utterance_id in utterance_ids:\n",
    "        if count>=10:\n",
    "            break\n",
    "\n",
    "        if \"_1.wav\" in utterance_id:\n",
    "            kaldi_dir = '/mnt/data1/waris/datasets/mandarin_bi_speakers/English/'+speaker+'/kaldi'\n",
    "            ki = KaldiInterface(wav_scp=str(os.path.join(kaldi_dir, 'wav.scp')),\n",
    "                                bnf_scp=str(os.path.join(kaldi_dir, 'bnf/feats.scp')))\n",
    "            bnf = ki.get_feature('_'.join([speaker, utterance_id[:-4]]), 'bnf')\n",
    "\n",
    "            acc_utterance_path = '/mnt/data1/waris/datasets/data/arctic_dataset/BDL/wav/arctic_b0534.wav'\n",
    "            embed_accent = generate_accent_embed(acc_utterance_path)\n",
    "            bnf_native = translate_ppg(bnf, embed_accent)\n",
    "\n",
    "            tgt_utterance_path = '/mnt/data1/waris/datasets/mandarin_bi_speakers/English/'+speaker+'/wav/'+str(utterance_id)\n",
    "            embed_speaker = generate_speaker_embed(tgt_utterance_path)\n",
    "\n",
    "            synthesis_wav = synthesize(bnf_native, embed_speaker)\n",
    "\n",
    "            output_dir = '/home/grads/q/quamer.waris/projects/ac-vc/synthesis_output/crosslingual_analysis/'+str(speaker)\n",
    "            if not os.path.isdir(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            filename = str(utterance_id)\n",
    "            output_file = os.path.join(output_dir, filename)\n",
    "            wavfile.write(output_file, hparams.sample_rate, synthesis_wav)\n",
    "\n",
    "            embed_speaker = generate_speaker_embed(output_file)\n",
    "            speaker_embeddings.append(embed_speaker)\n",
    "            speaker_labels.append(\"ENG_MAN\")\n",
    "            count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PCA projection\n",
      "Computing t-SNE embedding\n"
     ]
    }
   ],
   "source": [
    "### Computing PCA\n",
    "print(\"Computing PCA projection\")\n",
    "X_pca = decomposition.TruncatedSVD(n_components=2).fit_transform(speaker_embeddings)\n",
    "## Computing t-SNE\n",
    "print(\"Computing t-SNE embedding\")\n",
    "tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)\n",
    "speaker_tsne = tsne.fit_transform(speaker_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['tab:blue', 'tab:green', 'tab:red', 'tab:gray', 'tab:brown']\n",
    "shapes = [\"o\", \"^\", \"*\", \"s\", \"D\"]\n",
    "labels = [\"ENG\", \"JAP\", \"MAN\", \"MAN_ENG\", \"ENG_MAN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 2) ENG\n",
      "(600, 2) JAP\n",
      "(600, 2) MAN\n",
      "(60, 2) MAN_ENG\n",
      "(60, 2) ENG_MAN\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.cm as cm\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "for speaker, c, m in zip(labels, colors, shapes):\n",
    "    X_speaker_embedding = X_pca[np.where(speaker==np.array(speaker_labels))]\n",
    "    print(X_speaker_embedding.shape, speaker)\n",
    "    plt.scatter(X_speaker_embedding[:,0], X_speaker_embedding[:,1], label=speaker, marker=m, color=c)\n",
    "    # plt.scatter(speaker_embeddings[:, 0], speaker_embeddings[:, 1], c=speakers_all)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"SpeakerEmbeddings_Jap_Eng_BiMan_Corrected_PCA.png\", format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 2) ENG\n",
      "(600, 2) JAP\n",
      "(600, 2) MAN\n",
      "(60, 2) MAN_ENG\n",
      "(60, 2) ENG_MAN\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.cm as cm\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "for speaker, c, m in zip(labels, colors, shapes):\n",
    "    X_speaker_embedding = speaker_tsne[np.where(speaker==np.array(speaker_labels))]\n",
    "    print(X_speaker_embedding.shape, speaker)\n",
    "    plt.scatter(X_speaker_embedding[:,0], X_speaker_embedding[:,1], label=speaker, marker=m, color=c)\n",
    "    # plt.scatter(speaker_embeddings[:, 0], speaker_embeddings[:, 1], c=speakers_all)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"SpeakerEmbeddings_Jap_Eng_BiMan_Corrected_TSNE.png\", format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21646/612215341.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mroot_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/mnt/data1/waris/model_results/vc_vctk_samples/converted_utterances/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mspeaker\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdir_speakers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutterances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mspeaker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtarget_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/mnt/data1/waris/model_results/vc_vctk_samples/target_split/seen/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mspeaker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "_, dir_speakers, _ = next(os.walk('/mnt/data1/waris/model_results/vc_vctk/converted_utterances'))\n",
    "\n",
    "root_path = '/mnt/data1/waris/model_results/vc_vctk_samples/converted_utterances/'\n",
    "for speaker in dir_speakers:\n",
    "    _, _, utterances = next(os.walk(root_path+speaker))\n",
    "\n",
    "    target_dir = '/mnt/data1/waris/model_results/vc_vctk_samples/target_split/seen/'+speaker\n",
    "    if not os.path.isdir(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "\n",
    "    for utterance in utterances:\n",
    "        src_path = '/mnt/data1/waris/datasets/vctk/wav48_silence_trimmed/'+speaker+\"/wav/\"+speaker+\"_\"+utterance\n",
    "        shutil.copy(src_path, target_dir)\n",
    "\n",
    "    o_path, _ , orginal_utterances = next(os.walk('/mnt/data1/waris/datasets/vctk/wav48_silence_trimmed/'+speaker+'/wav/'))\n",
    "\n",
    "    target_dir = '/mnt/data1/waris/model_results/vc_vctk_samples/target_split/unseen/'+speaker\n",
    "    if not os.path.isdir(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "    count = 0\n",
    "    for utterance in orginal_utterances:\n",
    "        if count >= 50:\n",
    "            break\n",
    "        if \"_mic2.wav\" in utterance and utterance[-12:] not in utterances:\n",
    "            shutil.copy(o_path+utterance, target_dir)\n",
    "            count = count+1\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "# _, _, src_utterances = next(os.walk(\"/mnt/data1/waris/datasets/vctk/wav48_silence_trimmed/p250/wav/\"))\n",
    "\n",
    "# _, _, used_utterances = next(os.walk(\"/mnt/data1/waris/model_results/vc_vctk_samples/source_utterances/\"))\n",
    "\n",
    "# count = 0\n",
    "# for utterance in src_utterances:\n",
    "#     if count >=50:\n",
    "#         break\n",
    "#     if \"_mic2.wav\" in utterance and utterance not in used_utterances:\n",
    "#         shutil.copy('/mnt/data1/waris/datasets/vctk/wav48_silence_trimmed/p250/wav/'+utterance, '/mnt/data1/waris/model_results/vc_vctk/source_utterances')\n",
    "#         count = count + 1\n",
    "# print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2):\n",
    "    return list(set(lst1) & set(lst2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['039_mic2.wav', '282_mic2.wav', '271_mic2.wav', '298_mic2.wav', '063_mic2.wav', '134_mic2.wav', '205_mic2.wav', '242_mic2.wav', '308_mic2.wav', '212_mic2.wav', '002_mic2.wav', '069_mic2.wav', '111_mic2.wav', '302_mic2.wav', '222_mic2.wav', '005_mic2.wav']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "_, speaker_dirs , _ = next(os.walk('/mnt/data1/waris/model_results/vc_vctk/converted_utterances'))\n",
    "\n",
    "for speaker in speaker_dirs:\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/mnt/data1/waris/model_results/vctk_vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "11fd1a126a9d23a1622e8d2e6b4646d7e8bce1cbb7cad26eee91850269d70351"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('autospeech')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
